# -*- coding: utf-8 -*-
# Coordinated dual-station simulation for bike-sharing
# This script runs a simulation using the PySD library to interact with a Vensim model.
# Originally created in a Jupyter Notebook environment.
"""Coordinated dual-station simulation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11lj7d6MW3oWKy8xGgyp_W5fjhf1ylYkS
"""


# Check if PySD library is installed; install if not found
pip_show_output = !pip show pysd
slist_str = "\n".join(pip_show_output)

if slist_str.find("not found") != -1:
    !pip install pysd

"""### Global declarations"""


# Define global paths for data, forecasting models, and Vensim models
DATA_FOLDER = "/gdrive/MyDrive/Colab Notebooks/Bike share case study/Data/"
FC_MODELS_FOLDER = "/gdrive/MyDrive/Colab Notebooks/Bike share case study/Models/"
SIM_MODELS_FOLDER = "/gdrive/MyDrive/Colab Notebooks/Bike share case study/Vensim models/"
FORECASTS_FOLDER = "/gdrive/MyDrive/Colab Notebooks/Bike share case study/Data/Predictions/"

MODEL_FILES_PATTERN = "<scode>_<dtype>"
DEMAND_FILES_PATTERN = "Outlier corrected bikeshare base demands Aug_1S <scode>_UTL0.95.csv"
FORECAST_FILES_PATTERN = "1-day-ahead forecast <scode> v2.csv"

CSV_COLS_TO_READ = ["Datetime", "Rental count outlier correction", "Return count outlier correction"]
DEMAND_COLS_ORIG = ["Rentals", "Returns"]
DEMAND_COLS_YIN = ["Rentals YIN", "Returns YIN"]
DEMAND_COLS_YANG = ["Rentals YANG", "Returns YANG"]
TIME_SIGNAL_COLS = []

HEADER_COLS_FCFILE = ["Date", "Station"]
DEMAND_COLS_FCFILE = ["Rentals", "Rentals original", "Returns", "Returns original"]
FC_COLS = ["Rental predictions", "Return predictions"]

MODELS_DICT = {
    "LOOKAHEAD_YIN": (None, None, None, None), # sim, rentals, returns, and scaler
    "LOOKAHEAD_YANG": (None, None, None, None),
    "ACTUAL_YIN_YANG": (None, None, None, None) # sim, rentals, returns, scaler YIN
}
CURRENT_MODEL = ""
RT_MODEL = ""
FC_BEYOND_FCLEN = 0 # Current count of look-ahead period minus current period; initialize to zero
FCB_PROCESSING = True # Use FC to substitute demands that overshoot current time in rea-time simulation
FCB_VERBOSE = False

USE_SCENARIO_TRAINED = False

HIST_LEN = 2 * 24 * 6 # In number of 10-minute periods
DAY_LEN = 1 * 24 * 6 # Day in number of 10-minute periods
FC_LEN = 2 * 6 # In number of 10-minute periods

STATION_CAPACITY = {
    "BE19": 45,
    "BE21": 54,
    "E16": 65,
    "E16I": 79,
    "E17": 66,
    "E20": 45,
    "W20": 57
}

PLOT_DASHBOARDS = False
SAVE_SCENARIO_DEMANDS = False
SAVE_PBAF = True
SAVE_SCENARIO_RESULTS = False
COLLATE_SCENARIO_OUTPUTS = False

from google.colab import drive
drive.mount("/gdrive")

!pip install scikit-learn==1.3.2


# Import necessary libraries
import pysd
from pysd.py_backend.output import ModelOutput

# Import necessary libraries
import numpy as np

# Import necessary libraries
import pandas as pd
from datetime import datetime

# Import necessary libraries
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Import necessary libraries
import plotly.express as px
from tensorflow import keras

# Import necessary libraries
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler

# Import necessary libraries
import joblib

# Import necessary libraries
import matplotlib.pyplot as plt
from scipy import stats

# Import necessary libraries
import itertools
from copy import deepcopy
from tqdm.notebook import tqdm

# Import necessary libraries
import sys

# Import necessary libraries
import glob

sys.path.append("/gdrive/MyDrive/Colab Notebooks/Bike share case study/")
from custom_package.custom_layer import BiasCorrectionLayer
from custom_package.custom_loss import AsymmetricHybridLoss

"""##Steps to implement the calculation of the expected bike availability factor for a given planning horizon

Prerequisites:
1. The trained ML forecasting model for the station is saved to a file.
2. The scaler used to scale the features of the training data is saved to a file.

Steps:
1. Load the rentals and returns for the station.
2. Apply sine and cosine transformations on the date-time field to create the covariates (the same logic used to generate the training inputs for the model).
3. Write a function that takes as input the features and generates predictions for the following n periods where n=forecast horizon.
4. Write a function that loads the Vensim simulation model.
5. Override the lookup functions in the simulation model that pass the actual demands and predictions. Predictors' overridden functions should take the time variable as input and pass the appropriate time series and covariates as inputs to generate predictions. They should return the prediction for the current period.
6. Run the simulation once for the entire day with only the initial stock synchronised (no additional synchronisations). The resulting Expected Bike Availability Factors (EBAF) are cached. We use the cached factors as default values for periods that belong to batches for which replanning has not happened.

# Simulation-related functions
7. Write a function that simulates a specified batch number - the batch number denotes the time buckets (=length of the forecast horizon) for which the simulation should generate EBAF.
8. Another model object refers to the online simulation of the real-world stock evolution at the paired station clusters (YIN-YANG).
9. The two models are run one after the other for each batch. The first model generates the EBAF for the forthcoming batch planning horizon, and taking this as the input, the second model calculates an outlook of EBAF (e.g., average utilisation over the following n periods) that is an input for determining the risk of outage and the initiation of incentive actions.

## Helpers for visualization

### Dashboard
"""

def plot_dashboard(data, metrics_groups, y_axis_labels, x_axis_label="Hours", tick_interval="hourly", title="Time-series Dashboard", x_axis_font_size=10, y_axis_font_size=10, y_axis_standoff=3):
    # Ensure 'Date' is not an index level and avoid ambiguity
    data = data.copy()
    if data.index.name == "Date":
        data.reset_index(inplace=True)
    data.rename(columns={"Date": "DateTime"}, inplace=True)

    # Extract the scenario key if it exists
    if "scenario_key" not in data.columns:
        raise ValueError("The data must include a 'scenario_key' column.")

    # Extract date part from the 'DateTime' column
    data["Date"] = data["DateTime"].dt.date.astype(object)

    # Group data by date and scenario
    grouped_data = data.groupby(["Date", "scenario_key"])

    # Calculate the number of unique day-scenario pairs
    unique_day_scenario_pairs = grouped_data.size().reset_index(name="counts")
    num_day_scenario_pairs = len(unique_day_scenario_pairs)

    num_metrics_groups = len(metrics_groups)

    # Define colors for each unique metric
    unique_metrics = list(set(metric for group in metrics_groups for metric in group))
    color_palette = px.colors.qualitative.Plotly
    color_map = {metric: color_palette[i % len(color_palette)] for i, metric in enumerate(unique_metrics)}

    # Calculate dynamic vertical spacing
    max_rows = 1 / 0.1  # Max number of rows for vertical_spacing of 0.1
    if num_day_scenario_pairs > max_rows:
        vertical_spacing = np.round(1 / num_day_scenario_pairs, 2) - 0.005 # 0.005 works for about 80 rows; for fewer rows (~30), 0.01 might work
    else:
        vertical_spacing = 0.3 # Value between 0.1 and 0.3, depending upon the number of rows might work

    # Fixed row height
    row_height = 400  # Adjust as needed
    total_height = row_height * num_day_scenario_pairs + 200  # Add extra space for the title and legend

    # Function to wrap text
    def wrap_text(text, width):
        words = text.split()
        wrapped_text = ""
        line_length = 0
        for word in words:
            if line_length + len(word) > width:
                wrapped_text += "<br>"
                line_length = 0
            wrapped_text += word + " "
            line_length += len(word) + 1
        return wrapped_text.strip()

    # Create a figure for all subplots
    fig = make_subplots(
        rows=num_day_scenario_pairs, cols=num_metrics_groups,
        subplot_titles=[wrap_text(f"day:{date.day}, {scenario_key}", 30) for (date, scenario_key), _ in grouped_data for _ in range(num_metrics_groups)],
        shared_xaxes=False, vertical_spacing=vertical_spacing, horizontal_spacing=0.06
    )

    # Iterate over each day and scenario
    row_index = 1
    for (date, scenario_key), day_data in grouped_data:
        col_index = 1
        # Iterate over each group of metrics
        for metrics, y_label in zip(metrics_groups, y_axis_labels):
            for metric in metrics:
                fig.add_trace(
                    go.Scatter(x=day_data["DateTime"].values, y=day_data[metric], mode="lines", name=metric,
                               line=dict(color=color_map[metric]),
                               hovertemplate="%{data.name}<br>Time: %{x|%H:%M}<br>Value: %{y}", showlegend=(metric not in [trace.name for trace in fig.data])),
                    row=row_index, col=col_index
                )
            # Update x-axis to limit to the current day's data
            fig.update_xaxes(row=row_index, col=col_index, range=[day_data["DateTime"].values.min(), day_data["DateTime"].values.max()])
            # Add y-axis label
            fig.update_yaxes(title_text=y_label, row=row_index, col=col_index, title_standoff=y_axis_standoff)
            if row_index > 1:
                fig.update_yaxes(matches=f'y{col_index}', row=row_index, col=col_index)
            col_index += 1
        row_index += 1

    # Set tick interval
    if tick_interval == "hourly":
        dtick_value = 3600000  # Every hour in milliseconds
        tickformat = "%H:%M"  # Display only hour and minute
    elif tick_interval == "daily":
        dtick_value = "D1"  # Daily ticks
        tickformat = "%d-%m %H:%M"
    elif tick_interval == "weekly":
        dtick_value = "W1"  # Weekly ticks
        tickformat = "%d-%m %H:%M"
    else:
        dtick_value = "M1"  # Default to monthly ticks
        tickformat = "%d-%m %H:%M"

    fig.update_xaxes(
        tickformat=tickformat,
        dtick=dtick_value,
        ticklabelmode="period",
        title_text=x_axis_label,
        tickangle=-90,
        tickfont=dict(size=x_axis_font_size)
    )

    fig.update_yaxes(
        tickfont=dict(size=y_axis_font_size)
    )

    # Update layout and display
    fig.update_layout(
        height=total_height,
        width=1600,
        title_text=title,
        title_y=0.995, # If rows ~80, use a value like 0.998, else 0.995
        showlegend=True,
        legend=dict(orientation="v", yanchor="top", y=1, xanchor="left", x=1.02, font=dict(size=10)),  # Position the legend below the title
        template="plotly_white",
        legend_traceorder="normal",
        legend_itemwidth=50,
        margin=dict(t=250)
    )

    # Update the font size of the subplot titles
    for annotation in fig["layout"]["annotations"]:
        annotation["font"] = dict(size=12)  # Adjust the font size of subplot titles

    fig.show()

"""### Other plotting functions"""

def plot_data(data, columns_to_plot, subplots=False, tick_interval="weekly", title="Time-series data"):
    if not subplots:
        fig = go.Figure()
        for col in columns_to_plot:
            fig.add_trace(go.Scatter(x=data.index, y=data[col], mode="lines", name=col, hovertemplate="%{data.name}<br>Time: %{x|%H:%M}<br>Value: %{y}"))
    else:
        fig = make_subplots(rows=len(columns_to_plot), cols=1, shared_xaxes=True)
        for i, col in enumerate(columns_to_plot):
            fig.add_trace(
                go.Scatter(x=data.index, y=data[col], mode="lines", name=col, hovertemplate="%{data.name}<br>Time: %{x|%H:%M}<br>Value: %{y}"),
                row=i+1, col=1
            )

    # Set tick interval
    if tick_interval == "hourly":
        dtick_value = 3600000 * 3  # Every 3 hours in milliseconds
    elif tick_interval == "daily":
        dtick_value = "D1"  # Daily ticks
    elif tick_interval == "weekly":
        dtick_value = "W1"  # Weekly ticks
    else:
        dtick_value = "M1"  # Default to monthly ticks

    fig.update_xaxes(
        tickformat="%d-%m %H:%M",
        dtick=dtick_value,
        ticklabelmode="period"
    )
    fig.update_layout(
        height=200 * len(columns_to_plot),
        title_text=title,
        showlegend=True,
        template="plotly_white"
    )
    fig.show()

def plot_data_compare(df, original_col_pair, adjusted_col_pair, subtitles, title):
    orig_col_a, orig_col_b = original_col_pair
    adjusted_col_a, adjusted_col_b = adjusted_col_pair
    subtitle_a, subtitle_b = subtitles
    fig = make_subplots(rows=2, cols=1, subplot_titles=(subtitle_a, subtitle_b))

    # Process each type of data (rentals and returns)
    for idx, (orig_col, adjusted_col, subtitle) in enumerate(
        [(df[orig_col_a], df[adjusted_col_a], subtitle_a),
         (df[orig_col_b], df[adjusted_col_b], subtitle_b)], start=1):

        # Add original and selected scenario lines
        fig.add_trace(
            go.Scatter(x=df.index, y=orig_col, mode="lines", name=f"Original {subtitle}",
                       line=dict(color="blue" if subtitle == subtitle_a else "red", width=2)),
            row=idx, col=1
        )
        fig.add_trace(
            go.Scatter(x=df.index, y=adjusted_col, mode="lines", name=f"Scenario {subtitle}",
                       line=dict(color="blue" if subtitle == subtitle_a else "red", width=2, dash="dot")),
            row=idx, col=1
        )

        # Iterate over the DataFrame to draw vertical lines
        for x, y_orig, y_adj in zip(df.index, orig_col, adjusted_col):
            if y_adj > y_orig:
                # Green line if adjusted is above original
                fig.add_trace(
                    go.Scatter(x=[x, x], y=[y_orig, y_adj], mode="lines",
                               line=dict(color="green", width=1), showlegend=False),
                    row=idx, col=1
                )
            elif y_adj < y_orig:
                # Red line if adjusted is below original
                fig.add_trace(
                    go.Scatter(x=[x, x], y=[y_orig, y_adj], mode="lines",
                               line=dict(color="red", width=1), showlegend=False),
                    row=idx, col=1
                )

    # Add dummy traces for legend (only once for the entire chart)
    fig.add_trace(
        go.Scatter(x=[None], y=[None], mode="lines", name="Scenario > Original",
                   line=dict(color="green", width=1))
    )
    fig.add_trace(
        go.Scatter(x=[None], y=[None], mode="lines", name="Scenario < Original",
                   line=dict(color="red", width=1))
    )

    # Update layout and display
    fig.update_layout(
        height=600,
        width=1000,
        title_text=title,
        legend_title="Legend"
    )
    fig.show()

def visualize_demand_scenarios(actual_demands_df, base_cols, rental_scenarios, return_scenarios, title):
    rental_col, return_col = base_cols
    base_rentals = actual_demands_df[rental_col].to_numpy()
    base_returns = actual_demands_df[return_col].to_numpy()
    periods = actual_demands_df.index
    num_periods = len(periods)

    # Ensure the scenario arrays are not longer than the number of periods in the DataFrame
    rental_scenarios = rental_scenarios[:num_periods, :] if rental_scenarios.shape[0] > num_periods else rental_scenarios
    return_scenarios = return_scenarios[:num_periods, :] if return_scenarios.shape[0] > num_periods else return_scenarios

    def calculate_dynamic_opacities(scenarios, actuals):
        deviation_from_actuals = np.abs(scenarios - actuals[:, np.newaxis])
        max_deviation = np.max(deviation_from_actuals, axis=0)
        opacities = 1 - (deviation_from_actuals / max_deviation)  # Normalize
        opacities = np.clip(opacities, 0.1, 0.6)  # Adjust min and max opacity as needed
        return np.median(opacities, axis=1)  # Median opacity for each scenario

    rental_opacities = calculate_dynamic_opacities(rental_scenarios, base_rentals)
    return_opacities = calculate_dynamic_opacities(return_scenarios, base_returns)

    # Create a subplot with 2 rows
    fig = make_subplots(rows=2, cols=1, subplot_titles=("Rental Demand Scenarios", "Return Demand Scenarios"))

    # Add rental demand scenarios and actual demands to the first subplot
    for scenario, opacity in zip(rental_scenarios.T, rental_opacities):
        fig.add_trace(
            go.Scatter(x=periods, y=scenario, mode="lines",
                       line=dict(width=0.5, color=f"rgba(100, 149, 237, {opacity})"),  # Cornflower blue
                       showlegend=False),
            row=1, col=1
        )
    fig.add_trace(
        go.Scatter(x=periods, y=base_rentals, mode="lines", name=rental_col,
                   line=dict(color="navy", width=2)),  # Darker blue for actual rentals
        row=1, col=1
    )

    # Add return demand scenarios and actual demands to the second subplot
    for scenario, opacity in zip(return_scenarios.T, return_opacities):
        fig.add_trace(
            go.Scatter(x=periods, y=scenario, mode="lines",
                       line=dict(width=0.5, color=f"rgba(255, 182, 193, {opacity})"),  # Light pink
                       showlegend=False),
            row=2, col=1
        )
    fig.add_trace(
        go.Scatter(x=periods, y=base_returns, mode="lines", name=return_col,
                   line=dict(color="darkred", width=2)),  # Darker red for actual returns
        row=2, col=1
    )

    # Update layout and show figure
    fig.update_layout(
        title=title,
        xaxis_title="Date",
        xaxis2_title="Date",  # Ensure the x-axis title is repeated for clarity
        yaxis_title="Demand (Rentals)",
        yaxis2_title="Demand (Returns)",  # Separate y-axis titles for clarity
        legend_title="Legend",
        template="plotly_white",
        height=800  # Adjust the height of the figure to accommodate both subplots
    )

    fig.show()

"""## Helpers for data preparation and provisioning for the simulations"""

def load_bikeshare_demands(station_code, source_cols, target_demand_cols):
  fpath = DATA_FOLDER + DEMAND_FILES_PATTERN.replace("<scode>", station_code)
  df = pd.read_csv(fpath, skip_blank_lines=True, usecols=source_cols, parse_dates=["Datetime"], dayfirst=True)
  df.columns = ["Date"] + target_demand_cols
  df = df.sort_values("Date").set_index("Date")
  df = df.dropna(how="all")
  return df

def generate_time_signals(df):
  hours_elapsed_in_day = df.index.hour / 24
  hours_elapsed_in_week = (df.index.day * 24 + df.index.hour)/(7*24)

  df["sin_time_of_day"] = np.sin(2 * np.pi * hours_elapsed_in_day)
  df["cos_time_of_day"] = np.cos(2 * np.pi * hours_elapsed_in_day)
  df["sin_time_of_week"] = np.sin(2 * np.pi * hours_elapsed_in_week)
  df["cos_time_of_week"] = np.cos(2 * np.pi * hours_elapsed_in_week)

  time_signal_cols = ["sin_time_of_day", "cos_time_of_day", "sin_time_of_week", "cos_time_of_week"]

  return df, time_signal_cols

def create_datetime_range(year, month, day_from_to):
  start_day, end_day = day_from_to

  start_date = datetime(year, month, start_day).strftime("%Y-%m-%d")
  start_date += " 00:00"

  end_date = datetime(year, month, end_day).strftime("%Y-%m-%d")
  end_date += " 23:50"

  return (start_date, end_date)


# Simulation-related functions
def load_model(station_code, is_scen_trained=False, is_rentals=True):
  scen_prefix = "scen_" if is_scen_trained else ""
  dtype = scen_prefix + "rentals" if is_rentals else scen_prefix + "returns"
  path = FC_MODELS_FOLDER + MODEL_FILES_PATTERN.replace("<scode>", station_code).replace("<dtype>", dtype) + ".keras"

# Simulation-related functions
  return keras.models.load_model(path, custom_objects={
      "BiasCorrectionLayer": BiasCorrectionLayer,
      "AsymmetricHybridLoss": AsymmetricHybridLoss
      })

def load_scaler(station_code, is_scen_trained=False, is_rentals=True):
  scen_prefix = "scen_" if is_scen_trained else ""
  dtype = scen_prefix + "rentals" if is_rentals else scen_prefix + "returns"
  path = FC_MODELS_FOLDER + MODEL_FILES_PATTERN.replace("<scode>", station_code).replace("<dtype>", dtype) + ".scaler"
  return joblib.load(path)

def load_objects_yy(station_codes):
    scode_YIN, scode_YANG = station_codes
    is_scen_trained = USE_SCENARIO_TRAINED

    rental_YIN, rental_YANG = (

# Simulation-related functions
        load_model(scode_YIN, is_scen_trained=is_scen_trained, is_rentals=True),

# Simulation-related functions
        load_model(scode_YANG, is_scen_trained=is_scen_trained, is_rentals=True)
    )

    return_YIN, return_YANG = (

# Simulation-related functions
        load_model(scode_YIN, is_scen_trained=is_scen_trained, is_rentals=False),

# Simulation-related functions
        load_model(scode_YANG, is_scen_trained=is_scen_trained, is_rentals=False)
    )

    scaler_YIN, scaler_YANG = (
        load_scaler(scode_YIN, is_scen_trained=is_scen_trained, is_rentals=True),
        load_scaler(scode_YANG, is_scen_trained=is_scen_trained, is_rentals=True)
    )

    return rental_YIN, return_YIN, rental_YANG, return_YANG, scaler_YIN, scaler_YANG

def transform_data(scaler, df, date_range, cols_to_scale):
  dt_from, dt_to = date_range
  df.loc[dt_from:dt_to, cols_to_scale] = scaler.transform(df.loc[dt_from:dt_to, cols_to_scale])
  return df

def rename_cols(df, old_cols, new_cols):
  rename_dict = {o:v for o, v in zip(old_cols, new_cols)}
  df = df.rename(columns=rename_dict)
  return df

@tf.function
def model_predict(model, input_norms):
    return model(input_norms, training=False)

def denorm_single_pred(pred_norm, scaler, is_rentals):
  pred = np.zeros((1, 2))
  if is_rentals == True:
    pred[0, 0] = pred_norm
  else:
    pred[0, 1] = pred_norm
  pred_denormed = scaler.inverse_transform(pred)[0, 0] if is_rentals else scaler.inverse_transform(pred)[0, 1]
  return np.maximum(0, np.round(pred_denormed, 2))

def get_inputs_start_dt(pred_target_dt, hist_len=HIST_LEN, fc_len=FC_LEN):
  preds_start_dtime = pd.to_datetime(pred_target_dt)
  delta_hours = (hist_len + fc_len)/6
  inputs_start_dtime = preds_start_dtime - pd.Timedelta(hours=delta_hours) + pd.Timedelta(minutes=10)
  return inputs_start_dtime

def get_inputs_end_dt(inputs_start_dt, hist_len=HIST_LEN):
  inputs_start_dtime = pd.to_datetime(inputs_start_dt)
  delta_hours = hist_len/6
  inputs_end_dtime = inputs_start_dtime + pd.Timedelta(hours=delta_hours) - pd.Timedelta(minutes=10)
  return inputs_end_dtime

def denorm_batch_preds(preds_norm, scaler, is_rentals):
  preds = np.zeros((len(preds_norm), 2))
  if is_rentals == True:
    preds[:, 0] = preds_norm
  else:
    preds[:, 1] = preds_norm
  preds_denormed = scaler.inverse_transform(preds)[:, 0] if is_rentals else scaler.inverse_transform(preds)[:, 1]
  return np.maximum(0, np.round(preds_denormed, 2))

def get_test_inputs(df, pred_target_dt, feature_cols, t_overshoot=0, pred_col_norm=None, is_rentals=True):
    # Fetch the start and end times for input data
    inputs_start_dt = get_inputs_start_dt(pred_target_dt)
    inputs_end_dt = get_inputs_end_dt(inputs_start_dt)

    # Extract input data based on the date range (as a vector)
    inputs = df.loc[inputs_start_dt:inputs_end_dt, feature_cols].to_numpy()

    # Debugging: Keep track of the number of swaps printed
    swap_print_count = 0
    max_print_swaps = 3  # Limit to printing 3 swaps for debugging purposes

    # Handle overshoot: replace overshot periods starting from batch end date
    if t_overshoot > 0 and pred_col_norm is not None:
        # Start from the batch end date and go back as far as t_overshoot requires
        for i in range(t_overshoot):
            overshoot_date = inputs_end_dt - pd.Timedelta(minutes=10 * i)  # Go backwards from the input end date

            # Get original value
            original_value = inputs[-(i+1), 0 if is_rentals else 1]
            # Get the prediction to replace it (from the normalized predictions in df)
            new_value = df.loc[overshoot_date, pred_col_norm]

            # Replace historical data point with corresponding prediction
            inputs[-(i+1), 0 if is_rentals else 1] = new_value

            if FCB_VERBOSE:
              # Print the debug message for swapped values (limit the number of printed swaps)
              if swap_print_count < max_print_swaps:
                  print(f"Swapped value for {overshoot_date}: original={original_value}, new={new_value}")
                  swap_print_count += 1

    return inputs

def get_batch_denorm_predictions(model, scaler, df_norm, feature_cols, pred_target_dts, pred_col, is_rentals):
    if FCB_PROCESSING:
      # Initialize overshoot with FC_BEYOND_FCLEN if applicable
      t_over_first = FC_BEYOND_FCLEN if FC_BEYOND_FCLEN > 0 else 0

    # Initialize list to accumulate all input_norms for the batch
    input_norms = []

    # Process each prediction date in the batch
    for i, dt in enumerate(pred_target_dts):
        if FCB_PROCESSING:
          # Compute t_overshoot for the current prediction date, incrementing as needed
          t_overshoot = t_over_first + i if t_over_first else 0
        else:
          t_overshoot = 0

        # Fetch inputs and handle any overshoot replacement
        input_norms_single = get_test_inputs(df_norm, dt, feature_cols, t_overshoot,
                                             pred_col_norm=pred_col + "_NORM", is_rentals=is_rentals)

        # Accumulate input_norms for the batch
        input_norms.append(input_norms_single)

    # Convert the list of input_norms to a numpy array (with batch size as the first dimension)
    input_norms = np.array(input_norms)

    # Run the model to get normalized predictions for the batch
    preds = model_predict(model, input_norms)[:, -1, -1]

    # Denormalize the predictions
    denormed_preds = denorm_batch_preds(preds, scaler, is_rentals=is_rentals)

    return denormed_preds, preds

def get_prediction_for_sim(model, scaler, df_norm, feature_cols, df_preds, pred_date, pred_col, is_rentals):
    # Check if the prediction is already present for the target date
    if pd.isna(df_preds.loc[pred_date, pred_col]):
        # Define the batch start and end dates for predictions
        batch_start_dt = pred_date
        batch_end_dt = pd.to_datetime(pred_date) + pd.Timedelta(minutes=110) # Forecast horizon

        # Get prediction dates for the batch
        pred_dates = df_preds.loc[batch_start_dt:batch_end_dt].index

        # Fetch the denormalized and normalized predictions for this batch
        denormed_preds, norm_preds = get_batch_denorm_predictions(model, scaler, df_norm, feature_cols, pred_dates,
                                                                  pred_col, is_rentals=is_rentals)

        # Update df_preds with the denormalized predictions
        df_preds.loc[batch_start_dt:batch_end_dt, pred_col] = denormed_preds

        # Update df_norm with the normalized predictions for future reference
        pred_col_norm = pred_col + "_NORM"
        df_norm.loc[batch_start_dt:batch_end_dt, pred_col_norm] = norm_preds.numpy()

    return df_preds.loc[pred_date, pred_col]

"""## Function overrides for the simulation"""

def get_rental_actual_YIN():
  model_sim, _, _, _ = MODELS_DICT[CURRENT_MODEL]

  t = model_sim.time()
  if t == DAY_LEN:
    return 0
  else:
    return DF_DEM_SCENARIOS.loc[PRED_TARGETS_SIM_DAY[t], "Rentals scenario YIN"]

def get_rental_actual_YANG():
  model_sim, _, _, _ = MODELS_DICT[CURRENT_MODEL]

  t = model_sim.time()
  if t == DAY_LEN:
    return 0
  else:
    return DF_DEM_SCENARIOS.loc[PRED_TARGETS_SIM_DAY[t], "Rentals scenario YANG"]

def get_return_actual_YIN():
  model_sim, _, _, _ = MODELS_DICT[CURRENT_MODEL]

  t = model_sim.time()
  if t == DAY_LEN:
    return 0
  else:
    return DF_DEM_SCENARIOS.loc[PRED_TARGETS_SIM_DAY[t], "Returns scenario YIN"]

def get_return_actual_YANG():
  model_sim, _, _, _ = MODELS_DICT[CURRENT_MODEL]

  t = model_sim.time()
  if t == DAY_LEN:
    return 0
  else:
    return DF_DEM_SCENARIOS.loc[PRED_TARGETS_SIM_DAY[t], "Returns scenario YANG"]

def get_rental_pred():
  global FC_BEYOND_FCLEN

  model_sim, model_ren, _, scaler = MODELS_DICT[CURRENT_MODEL]
  model_type = CURRENT_MODEL[CURRENT_MODEL.find("_") + 1:] # string after the first "_"

  kf = "Rental predictions " + model_type
  t = model_sim.time()
  if t == DAY_LEN:
    return 0
  else:
    if FCB_PROCESSING:
      # If prediction beyond forecast horizon, update count
      model_rt, _, _, _ = MODELS_DICT[RT_MODEL]
      t_rt = model_rt.time()
      fc_overshoot = t - t_rt - FC_LEN
      if fc_overshoot >= 0:
        FC_BEYOND_FCLEN += 1

    feature_cols = DEMAND_COLS_YANG if model_type == "YANG" else DEMAND_COLS_YIN
    feature_cols = feature_cols + TIME_SIGNAL_COLS
    return get_prediction_for_sim(model_ren, scaler, DF_DEM_NORM, feature_cols,
                                  DF_DEM_PREDS_SIM_DAY, PRED_TARGETS_SIM_DAY[t], kf, is_rentals=True)

def get_return_pred():
  model_sim, _, model_ret, scaler = MODELS_DICT[CURRENT_MODEL]
  model_type = CURRENT_MODEL[CURRENT_MODEL.find("_") + 1:] # string after the first "_"

  kf = "Return predictions " + model_type
  t = model_sim.time()
  if t == DAY_LEN:
    return 0
  else:
    feature_cols = DEMAND_COLS_YANG if model_type == "YANG" else DEMAND_COLS_YIN
    feature_cols = feature_cols + TIME_SIGNAL_COLS
    return get_prediction_for_sim(model_ret, scaler, DF_DEM_NORM, feature_cols,
                                  DF_DEM_PREDS_SIM_DAY, PRED_TARGETS_SIM_DAY[t], kf, is_rentals=False)

def get_attractiveness_rentals():
  model_sim, _, _, _ = MODELS_DICT[CURRENT_MODEL]
  bike_avail_factor = np.round(model_sim["Bike availability factor"], 2)
  return LOOKUP_ATT_REN.loc[bike_avail_factor, "Attractiveness rentals"]

def get_attractiveness_rentals_YANG():
  model_sim, _, _, _ = MODELS_DICT[CURRENT_MODEL]
  bike_avail_factor = np.round(model_sim["Bike availability factor at Y"], 2)
  return LOOKUP_ATT_REN.loc[bike_avail_factor, "Attractiveness rentals"]

def get_attractiveness_returns():
  model_sim, _, _, _ = MODELS_DICT[CURRENT_MODEL]
  dock_avail_factor = np.round(model_sim["Dock availability factor"], 2)
  return LOOKUP_ATT_RET.loc[dock_avail_factor, "Attractiveness returns"]

def get_attractiveness_returns_YANG():
  model_sim, _, _, _ = MODELS_DICT[CURRENT_MODEL]
  dock_avail_factor = np.round(model_sim["Dock availability factor at Y"], 2)
  return LOOKUP_ATT_RET.loc[dock_avail_factor, "Attractiveness returns"]

def get_oos_risk_bikes_YIN_20():
  model_sim, _, _, _ = MODELS_DICT[CURRENT_MODEL]
  risk_index = np.round(model_sim["Perceived bike availability factor less GLB"], 3)
  return LOOKUP_OOS_RISK.loc[risk_index, "OOSB-R0.2"]

def get_oos_risk_docks_YIN_20():
  model_sim, _, _, _ = MODELS_DICT[CURRENT_MODEL]
  risk_index = np.round(model_sim["GUB less Perceived bike availability factor"], 3)
  return LOOKUP_OOS_RISK.loc[risk_index, "OOSD-R0.2"]

def get_oos_risk_bikes_YIN_30():
  model_sim, _, _, _ = MODELS_DICT[CURRENT_MODEL]
  risk_index = np.round(model_sim["Perceived bike availability factor less GLB"], 3)
  return LOOKUP_OOS_RISK.loc[risk_index, "OOSB-R0.3"]

def get_oos_risk_docks_YIN_30():
  model_sim, _, _, _ = MODELS_DICT[CURRENT_MODEL]
  risk_index = np.round(model_sim["GUB less Perceived bike availability factor"], 3)
  return LOOKUP_OOS_RISK.loc[risk_index, "OOSD-R0.3"]

def get_oos_risk_bikes_YANG_20():
  model_sim, _, _, _ = MODELS_DICT[CURRENT_MODEL]
  risk_index = np.round(model_sim["Perceived bike availability factor less GLB at Y"], 3)
  return LOOKUP_OOS_RISK.loc[risk_index, "OOSB-R0.2"]

def get_oos_risk_docks_YANG_20():
  model_sim, _, _, _ = MODELS_DICT[CURRENT_MODEL]
  risk_index = np.round(model_sim["GUB less Perceived bike availability factor at Y"], 3)
  return LOOKUP_OOS_RISK.loc[risk_index, "OOSD-R0.2"]

def get_oos_risk_bikes_YANG_30():
  model_sim, _, _, _ = MODELS_DICT[CURRENT_MODEL]
  risk_index = np.round(model_sim["Perceived bike availability factor less GLB at Y"], 3)
  return LOOKUP_OOS_RISK.loc[risk_index, "OOSB-R0.3"]

def get_oos_risk_docks_YANG_30():
  model_sim, _, _, _ = MODELS_DICT[CURRENT_MODEL]
  risk_index = np.round(model_sim["GUB less Perceived bike availability factor at Y"], 3)
  return LOOKUP_OOS_RISK.loc[risk_index, "OOSD-R0.3"]

def get_projected_baf_YIN():
  model_sim, _, _, _ = MODELS_DICT[CURRENT_MODEL]
  if model_sim.time() == 0:
    return INIT_PBAF_YIN
  else:
    return PBAF_AGG_YIN.loc[model_sim.time()]

def get_projected_baf_YANG():
  model_sim, _, _, _ = MODELS_DICT[CURRENT_MODEL]
  if model_sim.time() == 0:
    return INIT_PBAF_YANG
  else:
    return PBAF_AGG_YANG.loc[model_sim.time()]

def insert_result_cols(source_df, target_df, src_cols, target_cols=None):
  target_cols = src_cols if target_cols == None else target_cols

  for i, name in enumerate(src_cols):
    target_df[target_cols[i]] = source_df.loc[1:, name].values

  return target_df

# lookup attractiveness returns and rentals
lookup_att_ret_fpath = SIM_MODELS_FOLDER + "attractiveness function returns.csv"
lookup_att_ren_fpath = SIM_MODELS_FOLDER + "attractiveness function rentals.csv"

LOOKUP_ATT_RET = pd.read_csv(lookup_att_ret_fpath, skip_blank_lines=True)
LOOKUP_ATT_RET = LOOKUP_ATT_RET.set_index("Dock availability")

LOOKUP_ATT_REN = pd.read_csv(lookup_att_ren_fpath, skip_blank_lines=True)
LOOKUP_ATT_REN = LOOKUP_ATT_REN.set_index("Bike availability")

plt.figure(figsize=(10,4))
plt.plot(LOOKUP_ATT_RET.index, LOOKUP_ATT_RET["Attractiveness returns"], label="Dock availability & returns", color="blue")
plt.plot(LOOKUP_ATT_REN.index, LOOKUP_ATT_REN["Attractiveness rentals"], label="Bike availability & rentals", color="red")

plt.title("Impact of availability on demands")
plt.xlabel("Availability factor")
plt.ylabel("Impact factor")
plt.legend()
plt.show()

# lookup Out-of-Stock risk curves for bikes and docks
lookup_oos_fpath = SIM_MODELS_FOLDER + "oos function combined.csv"

LOOKUP_OOS_RISK = pd.read_csv(lookup_oos_fpath, skip_blank_lines=True)
LOOKUP_OOS_RISK = LOOKUP_OOS_RISK.set_index("Delta")

plt.figure(figsize=(10,4))
plt.plot(LOOKUP_OOS_RISK.index, LOOKUP_OOS_RISK["OOSB-R0.2"], label="OOS risk bikes (buffer=20%)", color="blue")
plt.plot(LOOKUP_OOS_RISK.index, LOOKUP_OOS_RISK["OOSD-R0.2"], label="OOS risk docks (buffer=20%)", color="red")
plt.plot(LOOKUP_OOS_RISK.index, LOOKUP_OOS_RISK["OOSB-R0.3"], label="OOS risk bikes (buffer=30%)", color="blue", linestyle="dashed")
plt.plot(LOOKUP_OOS_RISK.index, LOOKUP_OOS_RISK["OOSD-R0.3"], label="OOS risk docks (buffer=30%", color="red", linestyle="dashed")

plt.title("Outage risk as a function of distance from desired availability")
plt.xlabel("Distance from desired availability")
plt.ylabel("Risk factor")
plt.legend()
plt.show()

# lookup causal impact of incentives
lookup_cimpact_fpath = SIM_MODELS_FOLDER + "uplift rentals and returns.csv"

lookup_cimpact = pd.read_csv(lookup_cimpact_fpath, skip_blank_lines=True)
lookup_cimpact.fillna(0, inplace=True)

plt.figure(figsize=(10,4))
plt.plot(lookup_cimpact.index, lookup_cimpact["R+ Uplift H (%)"], label="Return uplift %", color="blue")
plt.plot(lookup_cimpact.index, lookup_cimpact["R- Uplift H (%)"], label="Rental uplift %", color="red")

plt.title("Effect of incentives as a function of time of the day")
plt.xlabel("Period")
plt.ylabel("Uplift as a % of normal demand")
plt.legend()
plt.show()

"""## Helpers for the initialization of the simulation models (lookahead and online)"""

def fn_overrides_lookahead(la_model):

# Simulation-related functions
  la_model.components.simulated_rental_rate = get_rental_pred

# Simulation-related functions
  la_model.components.simulated_return_rate = get_return_pred
  la_model.components.user_perception_of_attractiveness_for_rentals = get_attractiveness_rentals
  la_model.components.user_perception_of_attractiveness_for_returns = get_attractiveness_returns
  return la_model

def fn_overrides_yinyang(yy_model, risk_buffer):

# Simulation-related functions
  yy_model.components.simulated_normal_rental_rate = get_rental_actual_YIN

# Simulation-related functions
  yy_model.components.simulated_normal_rental_rate_at_y = get_rental_actual_YANG

# Simulation-related functions
  yy_model.components.simulated_normal_return_rate = get_return_actual_YIN

# Simulation-related functions
  yy_model.components.simulated_normal_return_rate_at_y = get_return_actual_YANG
  yy_model.components.projected_bike_availability_factor = get_projected_baf_YIN
  yy_model.components.projected_bike_availability_factor_at_y = get_projected_baf_YANG
  yy_model.components.user_perception_of_attractiveness_for_rentals = get_attractiveness_rentals
  yy_model.components.user_perception_of_attractiveness_for_rentals_at_y = get_attractiveness_rentals_YANG
  yy_model.components.user_perception_of_attractiveness_for_returns = get_attractiveness_returns
  yy_model.components.user_perception_of_attractiveness_for_returns_at_y = get_attractiveness_returns_YANG
  if (risk_buffer == 0.3):
    yy_model.components.out_of_stock_risk_due_to_bike_unavailability = get_oos_risk_bikes_YIN_30
    yy_model.components.out_of_stock_risk_due_to_bike_unavailability_at_y = get_oos_risk_bikes_YANG_30
    yy_model.components.out_of_stock_risk_due_to_dock_unavailability = get_oos_risk_docks_YIN_30
    yy_model.components.out_of_stock_risk_due_to_dock_unavailability_at_y = get_oos_risk_docks_YANG_30
  else:
    yy_model.components.out_of_stock_risk_due_to_bike_unavailability = get_oos_risk_bikes_YIN_20
    yy_model.components.out_of_stock_risk_due_to_bike_unavailability_at_y = get_oos_risk_bikes_YANG_20
    yy_model.components.out_of_stock_risk_due_to_dock_unavailability = get_oos_risk_docks_YIN_20
    yy_model.components.out_of_stock_risk_due_to_dock_unavailability_at_y = get_oos_risk_docks_YANG_20
  return yy_model

def init_sims(la_sim_path, yy_sim_path, risk_buffer):
  model_lookahead = pysd.read_vensim(la_sim_path)
  model_actual_yy = pysd.read_vensim(yy_sim_path)
  model_lookahead = fn_overrides_lookahead(model_lookahead)
  model_actual_yy = fn_overrides_yinyang(model_actual_yy, risk_buffer)
  return model_lookahead, model_actual_yy

def init_la_params(capacities, use_attractiveness):
  capa_YIN, capa_YANG = capacities
  param_YIN = {"Capacity": capa_YIN, "Use attractiveness quotient": use_attractiveness}
  param_YANG = {"Capacity": capa_YANG, "Use attractiveness quotient": use_attractiveness}
  return param_YIN, param_YANG

def init_yy_params(capacities, delays_YIN, delays_YANG, use_attractiveness, use_incentives, incentive_thresh, risk_buffer, sync_freq):
  capa_YIN, capa_YANG = capacities
  risk_delay_YIN, avail_delay_YIN = delays_YIN
  risk_delay_YANG, avail_delay_YANG = delays_YANG
  params = {"Cluster capacity": capa_YIN, "Cluster capacity at Y": capa_YANG,
            "Perception delay": risk_delay_YIN, "Responsiveness to actual": avail_delay_YIN,
            "Perception delay at Y": risk_delay_YANG, "Responsiveness to actual at Y": avail_delay_YANG,
            "Use attractiveness quotient": use_attractiveness, "Use attractiveness quotient at Y": use_attractiveness,
            "Use incentives": use_incentives, "Pressure threshold for incentive authorization": incentive_thresh,
            "Synchronization frequency": sync_freq}
  if risk_buffer == 0.3:
    params.update({
                    "Goal for lower bound for bike availability or GLB": 0.3,
                    "Goal for upper bound for bike availability or GUB": 0.7,
                    "Goal for lower bound for bike availability or GLB at Y": 0.3,
                    "Goal for upper bound for bike availability or GUB at Y": 0.7})
  else:
    params.update({
                    "Goal for lower bound for bike availability or GLB": 0.2,
                    "Goal for upper bound for bike availability or GUB": 0.8,
                    "Goal for lower bound for bike availability or GLB at Y": 0.2,
                    "Goal for upper bound for bike availability or GUB at Y": 0.8})
  return params

def init_models(capacities, delays_YIN, delays_YANG, use_attractiveness_quotient, use_incentives,
                incentive_threshold, risk_buffer, sync_freq):
  la_params_YIN, la_params_YANG = init_la_params(capacities, use_attractiveness_quotient)
  yy_params = init_yy_params(capacities, delays_YIN, delays_YANG, use_attractiveness_quotient, use_incentives,
                             incentive_threshold, risk_buffer, sync_freq)
  return la_params_YIN, la_params_YANG, yy_params

def init_predictions(df):
  df["Rental predictions YIN"] = pd.Series(np.nan, index=df.index, dtype=float)
  df["Return predictions YIN"] = pd.Series(np.nan, index=df.index, dtype=float)
  df["Rental predictions YANG"] = pd.Series(np.nan, index=df.index, dtype=float)
  df["Return predictions YANG"] = pd.Series(np.nan, index=df.index, dtype=float)
  return df

def init_norm_predictions(df):
  # Normalized
  df["Rental predictions YIN_NORM"] = pd.Series(np.nan, index=df.index, dtype=float)
  df["Return predictions YIN_NORM"] = pd.Series(np.nan, index=df.index, dtype=float)
  df["Rental predictions YANG_NORM"] = pd.Series(np.nan, index=df.index, dtype=float)
  df["Return predictions YANG_NORM"] = pd.Series(np.nan, index=df.index, dtype=float)
  return df

def init_results_df(simulation_day, df_source):
  df_sim = df_source.loc[df_source.index.day == simulation_day].copy()
  df_sim = init_predictions(df_sim)
  return df_sim

def set_la_return_cols():
  return ["Projected stock", "Bike availability factor"]

def set_yy_return_cols(type="long"):
  cols = ["Out of Stock of bikes",
          "Out of Stock of bikes at Y",
          "Out of Stock of docks",
          "Out of Stock of docks at Y"]
  if type == "long":
    cols += ["Cluster yin bike stock",
              "Cluster yang or Y bike stock",
              "Bike availability factor",
              "Bike availability factor at Y",
              "Projected bike availability factor",
              "Projected bike availability factor at Y",
              "Perceived bike availability factor",
              "Perceived bike availability factor at Y",
              "Perception of risk of OOS due to bike unavailability",
              "Perception of risk of OOS due to bike unavailability at Y",
              "Perception of risk of OOS due to dock unavailability",
              "Perception of risk of OOS due to dock unavailability at Y",
              "Expected returns uplift",
              "Expected returns uplift at Y",
              "Expected rentals uplift",
              "Expected rentals uplift at Y",
              "Change to returns rate due to incentives",
              "Change to returns rate due to incentives at Y",
              "Change to rentals rate due to incentives",
              "Change to rentals rate due to incentives at Y",
              "User perception of attractiveness for returns",
              "User perception of attractiveness for returns at Y",
              "User perception of attractiveness for rentals",
              "User perception of attractiveness for rentals at Y",
              "Change to returns rate due to UPoA",
              "Change to returns rate due to UPoA at Y",
              "Change to rentals rate due to UPoA",
              "Change to rentals rate due to UPoA at Y"]
  return cols

"""## Helpers for generating demand scenarios"""

def load_bikeshare_forecasts(station_code, date_col="Date"):
    fpath = FORECASTS_FOLDER + FORECAST_FILES_PATTERN.replace("<scode>", station_code)
    df = pd.read_csv(fpath, skip_blank_lines=True, parse_dates=[date_col])
    df.columns = HEADER_COLS_FCFILE + DEMAND_COLS_FCFILE + FC_COLS
    df = df.sort_values("Date").set_index("Date")
    df = df.dropna(how="all")
    return df

def calculate_rmse(data, fc_cols, actual_cols, days):
    rental_rmse = []
    return_rmse = []
    rental_fc_col, return_fc_col = fc_cols
    rental_act_col, return_act_col = actual_cols
    for day in days:
        filtered_data = data[data.index.day == day]
        rental_errors = filtered_data[rental_fc_col] - filtered_data[rental_act_col]
        return_errors = filtered_data[return_fc_col] - filtered_data[return_act_col]
        rental_rmse.append(np.sqrt((rental_errors ** 2).mean()))
        return_rmse.append(np.sqrt((return_errors ** 2).mean()))
    return rental_rmse, return_rmse

def generate_demand_scenarios(forecasted_rentals, forecasted_returns, rental_rmse, return_rmse, confidence_interval=0.95, scenarios=100):
    periods = len(forecasted_rentals)
    lower_percentile = (1 - confidence_interval) / 2
    upper_percentile = 1 - lower_percentile

    rental_errors = np.random.normal(0, rental_rmse, (periods, scenarios))
    return_errors = np.random.normal(0, return_rmse, (periods, scenarios))


# Simulation-related functions
    simulated_rental_demands = forecasted_rentals[:, np.newaxis] + rental_errors

# Simulation-related functions
    simulated_return_demands = forecasted_returns[:, np.newaxis] + return_errors


# Simulation-related functions
    return np.round(np.maximum(0, simulated_rental_demands)), np.round(np.maximum(0, simulated_return_demands))

def select_and_store_scenario(scenarios, df, day, percentile_bandwidth, store_col):
    selected_scenarios = np.zeros((scenarios.shape[0],))  # Array for each period's selected scenario

    # Loop through each period (144 in total)
    for i in range(scenarios.shape[0]):
        period_scenarios = scenarios[i, :]  # Scenarios for this period
        lower_bound = np.percentile(period_scenarios, percentile_bandwidth[0] * 100)
        upper_bound = np.percentile(period_scenarios, percentile_bandwidth[1] * 100)

        # Filter scenarios within the specified percentile bounds
        valid_scenarios = period_scenarios[(period_scenarios >= lower_bound) & (period_scenarios <= upper_bound)]

        # Randomly select one scenario from the valid scenarios
        if valid_scenarios.size > 0:
            selected_scenarios[i] = np.round(np.random.choice(valid_scenarios))
        else:
            selected_scenarios[i] = np.nan  # Handle case where no scenarios are within bounds

    # Store the selected scenarios into the DataFrame
    df.loc[df.index.day == day, store_col] = selected_scenarios

def merge_frames(df_left, df_right, merge_cols, suffixes):
  merged_df = pd.merge(df_left[merge_cols], df_right[merge_cols], left_index=True, right_index=True, how="inner", suffixes=suffixes)
  return merged_df

def generate_and_store_demand_scenarios_YY(df, fc_cols, raw_dem_cols, corr_dem_cols, scen_cols,
                                           retain_orig_rental, retain_orig_return,
                                           simulation_days, num_scenarios, ci, percentile_bandwidth):
    rental_fc_col, return_fc_col = fc_cols
    rental_raw_col, return_raw_col = raw_dem_cols
    rental_corr_col, return_corr_col = corr_dem_cols
    rental_scen_col, return_scen_col = scen_cols
    pc_band_rental, pc_band_return = percentile_bandwidth[0], percentile_bandwidth[1]
    rental_scen_all = []
    return_scen_all = []

    rental_rmse, return_rmse = calculate_rmse(df, fc_cols, corr_dem_cols, simulation_days)

    for i, day in enumerate(simulation_days):
        rental_scen, return_scen = generate_demand_scenarios(
            df[df.index.day == day][rental_fc_col].to_numpy(),
            df[df.index.day == day][return_fc_col].to_numpy(),
            rental_rmse[i], return_rmse[i], ci, num_scenarios
        )
        rental_scen_all.append(rental_scen)
        return_scen_all.append(return_scen)

        if retain_orig_rental == "raw":
          df.loc[df.index.day == day, rental_scen_col] = df.loc[df.index.day == day, rental_raw_col].copy()
        elif retain_orig_rental == "corrected":
          df.loc[df.index.day == day, rental_scen_col] = df.loc[df.index.day == day, rental_corr_col].copy()
        else:
          select_and_store_scenario(rental_scen, df, day, pc_band_rental, rental_scen_col)

        if retain_orig_return == "raw":
          df.loc[df.index.day == day, return_scen_col] = df.loc[df.index.day == day, return_raw_col].copy()
        elif retain_orig_return == "corrected":
          df.loc[df.index.day == day, return_scen_col] = df.loc[df.index.day == day, return_corr_col].copy()
        else:
          select_and_store_scenario(return_scen, df, day, pc_band_return, return_scen_col)

    rental_scen_arr = np.vstack(rental_scen_all)
    return_scen_arr = np.vstack(return_scen_all)

    return rental_scen_arr, return_scen_arr

def init_scenario_cols(df):
  df["Rentals scenario YIN"] = pd.Series(np.nan, index=df.index, dtype=float)
  df["Returns scenario YIN"] = pd.Series(np.nan, index=df.index, dtype=float)
  df["Rentals scenario YANG"] = pd.Series(np.nan, index=df.index, dtype=float)
  df["Returns scenario YANG"] = pd.Series(np.nan, index=df.index, dtype=float)
  return df

def gen_demand_scenarios(station_codes, days_range, num_scenarios, confidence_interval, percentile_bandwidths, retain_logics):
  raw_dem_cols=[("Rentals original YIN", "Returns original YIN"), ("Rentals original YANG", "Returns original YANG")]
  corr_dem_cols=[("Rentals YIN", "Returns YIN"), ("Rentals YANG", "Returns YANG")]
  fc_cols=[("Rental predictions YIN", "Return predictions YIN"), ("Rental predictions YANG", "Return predictions YANG")]
  scen_cols=[("Rentals scenario YIN", "Returns scenario YIN"), ("Rentals scenario YANG", "Returns scenario YANG")]
  station_code_YIN, station_code_YANG = station_codes

  df_fc_YIN = load_bikeshare_forecasts(station_code_YIN, date_col="Date")
  df_fc_YANG = load_bikeshare_forecasts(station_code_YANG, date_col="Date")
  df_scenarios = merge_frames(df_fc_YIN, df_fc_YANG, DEMAND_COLS_FCFILE + FC_COLS, suffixes=(" YIN", " YANG"))
  df_scenarios = init_scenario_cols(df_scenarios)

  scenarios = []
  for i in range(len(raw_dem_cols)):
    rental_scenarios, return_scenarios = generate_and_store_demand_scenarios_YY(df=df_scenarios,
                                                                                  fc_cols=fc_cols[i],
                                                                                  raw_dem_cols=raw_dem_cols[i],
                                                                                  corr_dem_cols=corr_dem_cols[i],
                                                                                  scen_cols=scen_cols[i],
                                                                                  retain_orig_rental=retain_logics[i][0],
                                                                                  retain_orig_return=retain_logics[i][1],
                                                                                  simulation_days=days_range,
                                                                                  num_scenarios=num_scenarios,
                                                                                  ci=confidence_interval,
                                                                                  percentile_bandwidth=percentile_bandwidths[i])
    scenarios.append((rental_scenarios, return_scenarios))

  return df_scenarios, scenarios

def modify_demands(df, time_range, reduction_percent, operation="reduce", handle_rentals=True, handle_returns=True, rentals_col="Rentals", returns_col="Returns"):
    start_time, end_time = [pd.to_datetime(t).time() for t in time_range]  # Convert string times to datetime.time objects
    mask = (df.index.time >= start_time) & (df.index.time <= end_time)

    # Process rentals and returns separately as needed
    for col in ([rentals_col] if handle_rentals else []) + ([returns_col] if handle_returns else []):
        if operation == "reduce":
            # Calculate total demand to be reduced within the time range
            reduced_amount = df.loc[mask, col].sum() * (reduction_percent / 100)
            # Reduce the demands within the specified time range
            df.loc[mask, col] *= (1 - reduction_percent / 100)

            # Calculate total demand outside the time range for spreading the reduction
            total_demand_outside = df.loc[~mask, col].sum()
            # If there"s no demand outside the specified range to avoid division by zero
            if total_demand_outside > 0:
                # Calculate the additional demand to add to each period outside the time range
                additional_demand = (df.loc[~mask, col] / total_demand_outside) * reduced_amount
                # Spread the reduced amount over the remaining periods proportionally
                df.loc[~mask, col] += additional_demand

            # Apply final rounding after all adjustments
            df.loc[:, col] = np.round(df.loc[:, col])
        elif operation == "delete":
            # Set demands to zero within the specified time range
            df.loc[mask, col] = 0

def synchronize_with_scenario(df_orig, df_scen):
  col_mapping = {"Rentals YIN": "Rentals scenario YIN",
                 "Rentals YANG": "Rentals scenario YANG",
                 "Returns YIN": "Returns scenario YIN",
                 "Returns YANG": "Returns scenario YANG"}

  non_null_mask = df_scen[list(col_mapping.values())].notnull().all(axis=1)
  filtered_df_scen = df_scen[non_null_mask]
  matching_index = df_orig.index.intersection(filtered_df_scen.index)
  for df_orig_col, df_scen_col in col_mapping.items():
    df_orig.loc[matching_index, df_orig_col] = df_scen.loc[matching_index, df_scen_col]
  return df_orig

def prepare_demands_for_forecast(station_codes, days_range, month, year, scenario_sync, scenario_df):
  global TIME_SIGNAL_COLS
  station_code_YIN, station_code_YANG = station_codes

  # Retrieve model and scaler objects and store in the global variable
  model_ren_YIN, model_ret_YIN, model_ren_YANG, model_ret_YANG, scaler_YIN, scaler_YANG = load_objects_yy((station_code_YIN, station_code_YANG))

  # Load/preprocess YIN and YANG demand inputs for forecasting
  df_YIN = load_bikeshare_demands(station_code_YIN, source_cols=CSV_COLS_TO_READ, target_demand_cols=DEMAND_COLS_YIN)
  df_YANG = load_bikeshare_demands(station_code_YANG, source_cols=CSV_COLS_TO_READ, target_demand_cols=DEMAND_COLS_YANG)
  df_YIN, TIME_SIGNAL_COLS = generate_time_signals(df_YIN)
  df = df_YIN.join(df_YANG[DEMAND_COLS_YANG])
  df = df[DEMAND_COLS_YIN + DEMAND_COLS_YANG + TIME_SIGNAL_COLS]

  # If synchronize, set the actual demands (used as inputs for forecasting) equal to the demands of the generated scenarios
  if scenario_sync:
    df = synchronize_with_scenario(df, scenario_df)

  # Copy test data range and scale it
  test_data_start, test_data_end = create_datetime_range(year, month, days_range)
  df_norm = df.loc[test_data_start:test_data_end].copy()

  # Temporary rename for transformation of YIN
  df_norm = rename_cols(df_norm, DEMAND_COLS_YIN, DEMAND_COLS_ORIG)
  df_norm.loc[:, DEMAND_COLS_ORIG] = transform_data(scaler_YIN, df_norm,
                                                        date_range=(test_data_start, test_data_end), cols_to_scale=DEMAND_COLS_ORIG)
  df_norm = rename_cols(df_norm, DEMAND_COLS_ORIG, DEMAND_COLS_YIN)

  # Temporary rename for transformation of YANG
  df_norm = rename_cols(df_norm, DEMAND_COLS_YANG, DEMAND_COLS_ORIG)
  df_norm.loc[:, DEMAND_COLS_ORIG] = transform_data(scaler_YANG, df_norm,
                                                        date_range=(test_data_start, test_data_end), cols_to_scale=DEMAND_COLS_ORIG)
  df_norm = rename_cols(df_norm, DEMAND_COLS_ORIG, DEMAND_COLS_YANG)

  # Add prediction columns that retain normalized values (to use as substitutes for demands when look-ahead time exceeds current time)
  df_norm = init_norm_predictions(df_norm)

  return df, df_norm, (model_ren_YIN, model_ret_YIN), (model_ren_YANG, model_ret_YANG), (scaler_YIN, scaler_YANG)

"""## Helpers for running the coordinated simulation"""


# Simulation-related functions
def batch_simulate(model, start_time, batch_size, overrun_periods, init_stock_param, other_params, return_cols, verbose=False):
  final_time = start_time + batch_size + overrun_periods
  if verbose == True:
    print(f"S {start_time} - E {final_time}")

  result = model.run(
        initial_condition=(start_time, init_stock_param),
        final_time=final_time,
        params=other_params,
        return_columns=return_cols
    )
  next_start_time = start_time + batch_size

  return result, next_start_time

def calc_aggregated_factor(factor_series, col, start_idx, periods, trunc_periods, aggregation_logic="MA"):
    # Extract the relevant part of the series
    factor_series = factor_series[start_idx:][col]

    # Apply the specified aggregation logic
    if aggregation_logic == "MA":
        # Calculate moving average
        factor_series = factor_series.rolling(window=periods, min_periods=1).mean()
    elif aggregation_logic == "MIN":
        # Calculate minimum over the next N periods
        factor_series = factor_series.rolling(window=periods, min_periods=1).min()
    elif aggregation_logic == "MEDIAN":
        # Calculate median over the next N periods
        factor_series = factor_series.rolling(window=periods, min_periods=1).median()
    else:
        raise ValueError(f"Unknown aggregation logic: {aggregation_logic}")

    # Shift to align the output correctly
    factor_series = factor_series.shift(-(periods - 1))

    # Truncate the series if necessary
    if trunc_periods:
        factor_series = factor_series[:-trunc_periods]

    # Round the series to three decimal places
    factor_series = factor_series.round(3)

    return factor_series

def run_lookahead_sim(la_model, next_start_time, batch_size, overrun, start_stock_param, la_params, return_cols, results, periods, pbaf_agg, aggregation_logic="MA", verbose=False):
    if verbose:
        print(f"Starting stock at {next_start_time} is {start_stock_param['Projected stock']}")

    # Run the simulation

# Simulation-related functions
    result_single, next_start_time = batch_simulate(model=la_model, start_time=next_start_time,
                                                    batch_size=batch_size, overrun_periods=overrun,
                                                    init_stock_param=start_stock_param, other_params=la_params,
                                                    return_cols=return_cols, verbose=verbose)
    # Append the results to the list, handling overrun
    if overrun:
        results.append(result_single[1:-overrun])
    else:
        results.append(result_single[1:])

    # Calculate the aggregated factor using the specified logic
    pbaf_single_aggregated = calc_aggregated_factor(result_single,
                                                    col="Bike availability factor",
                                                    start_idx=1,
                                                    periods=periods,
                                                    trunc_periods=overrun,
                                                    aggregation_logic=aggregation_logic)

    pbaf_single_aggregated = pbaf_single_aggregated.ffill()

    # Concatenate pbaf_agg with the newly aggregated pbaf_single_aggregated
    if pbaf_single_aggregated.empty:
        return next_start_time, results, pbaf_agg
    elif pbaf_agg.empty:
        pbaf_agg = pbaf_single_aggregated
    else:
        pbaf_agg = pd.concat([pbaf_agg, pbaf_single_aggregated])

    return next_start_time, results, pbaf_agg

def run_coordinated_simulation(station_codes, la_model, yy_model, batch_size, n_batches,
                               delays_YIN, delays_YANG, use_attractiveness, use_incentives,
                               incentive_threshold, init_stocks, init_pba_factors, risk_buffer,
                               agg_logic, agg_periods, df_results, tqdm_bar, verbose=False):
  global CURRENT_MODEL
  global RT_MODEL
  global PBAF_AGG_YIN, PBAF_AGG_YANG
  global FC_BEYOND_FCLEN

  # Store name of the real-time simulation model
  RT_MODEL = "ACTUAL_YIN_YANG"

  # Initialize parameters for lookahead and online models
  station_code_YIN, station_code_YANG = station_codes
  capacities = (STATION_CAPACITY[station_code_YIN], STATION_CAPACITY[station_code_YANG])
  la_params_YIN, la_params_YANG, yy_params = init_models(capacities=capacities,
                                                          delays_YIN=delays_YIN, delays_YANG=delays_YANG,
                                                          use_attractiveness_quotient=use_attractiveness,
                                                          use_incentives=use_incentives,
                                                          incentive_threshold=incentive_threshold,
                                                          risk_buffer=risk_buffer, sync_freq=batch_size)

  # Initialize YIN-YANG simulation
  CURRENT_MODEL = "ACTUAL_YIN_YANG"
  init_stock_YIN, init_stock_YANG = init_stocks
  init_pbaf_YIN, init_pbaf_YANG = init_pba_factors
  yy_init_stocks_YIN_YANG = {"Cluster yin bike stock": init_stock_YIN,
                       "Perceived bike availability factor": init_pbaf_YIN,
                       "Cluster yang or Y bike stock": init_stock_YANG,
                       "Perceived bike availability factor at Y": init_pbaf_YANG}
  yy_return_cols = set_yy_return_cols()
  num_preds = DAY_LEN
  yy_output = ModelOutput()
  yy_model.set_stepper(yy_output,
                        params=yy_params,
                        return_columns=yy_return_cols,
                        initial_condition=(0, yy_init_stocks_YIN_YANG),
                        final_time=num_preds)
  CURRENT_MODEL = ""

  results_list_YIN = []
  results_list_YANG = []
  results_list_YY = []
  next_start_time_YIN = 0
  next_start_time_YANG = 0
  la_return_cols = set_la_return_cols()
  for i in range(n_batches):
      if i == 0:
        la_start_stock_param_YIN = {"Projected stock": init_stock_YIN} # Optimal stock (YIN/E16)
        la_start_stock_param_YANG = {"Projected stock": init_stock_YANG} # Optimal stock (YANG/E16I)

      # Run lookahead YIN
      CURRENT_MODEL = "LOOKAHEAD_YIN"
      FC_BEYOND_FCLEN = 0 # Reset count at the start of each batch/cluster
      overrun = 0 if i == n_batches - 1 else 12

      if verbose:
        print(f"Current time of RT model is {yy_model.time()}")

      next_start_time_YIN, results_list_YIN, PBAF_AGG_YIN = run_lookahead_sim(la_model=la_model,
                                                                            next_start_time=next_start_time_YIN,
                                                                            batch_size=batch_size,
                                                                            overrun=overrun,
                                                                            start_stock_param=la_start_stock_param_YIN,
                                                                            la_params=la_params_YIN,
                                                                            return_cols=la_return_cols,
                                                                            results=results_list_YIN,
                                                                            aggregation_logic=agg_logic,
                                                                            periods=agg_periods,
                                                                            pbaf_agg=PBAF_AGG_YIN,
                                                                            verbose=verbose)
      if verbose:
        print(f"FC horizon overshot so far {FC_BEYOND_FCLEN} times")

      # Run lookahead YANG
      CURRENT_MODEL = "LOOKAHEAD_YANG"
      FC_BEYOND_FCLEN = 0 # Reset count at the start of each batch/cluster
      next_start_time_YANG, results_list_YANG, PBAF_AGG_YANG = run_lookahead_sim(la_model=la_model,
                                                                            next_start_time=next_start_time_YANG,
                                                                            batch_size=batch_size,
                                                                            overrun=overrun,
                                                                            start_stock_param=la_start_stock_param_YANG,
                                                                            la_params=la_params_YANG,
                                                                            return_cols=la_return_cols,
                                                                            results=results_list_YANG,
                                                                            aggregation_logic=agg_logic,
                                                                            periods=agg_periods,
                                                                            pbaf_agg=PBAF_AGG_YANG,
                                                                            verbose=verbose)
      if verbose:
        print(f"FC horizon overshot so far {FC_BEYOND_FCLEN} times")

      # Run YIN_YANG
      CURRENT_MODEL = "ACTUAL_YIN_YANG"
      yy_model.step(batch_size)

      la_start_stock_param_YIN = {"Projected stock": yy_model["Cluster yin bike stock"]}
      la_start_stock_param_YANG = {"Projected stock": yy_model["Cluster yang or Y bike stock"]}
      tqdm_bar.update(1)

  la_return_cols_YY = []
  for i in range(len(results_list_YIN)):
    df_left, df_right = results_list_YIN[i], results_list_YANG[i]
    results_list_YY.append(merge_frames(df_left, df_right, la_return_cols, ("_YIN", "_YANG")))
    if i == 0:
      la_return_cols_YY = results_list_YY[0].columns
  la_results = pd.concat(results_list_YY)

  yy_results = yy_output.collect(yy_model)

  df_results = insert_result_cols(la_results, df_results, src_cols=la_return_cols_YY)
  df_results = insert_result_cols(yy_results, df_results, src_cols=yy_return_cols)

  return df_results

"""## Coordinated simulation runs

### Scenario preliminaries
"""

# Top-level parameters
sim_params = {
    "station_codes_YY": ("E16", "E16I"),
    "test_days": (19, 25),
    "test_my": (8, 2023),
    "scen_days": [22, 23, 24, 25],
    "sync_fcdem_scen": True,
    "num_scenarios": 300,
    "confidence_interval": 0.95,
    "percentile_ranges_YIN": [(0.0, 1.0), (0.0, 1.0)], # rental, return
    "percentile_ranges_YANG": [(0.0, 1.0), (0.0, 1.0)], # rental, return
    "retain_logics": [("corrected", "corrected"), ("corrected", "corrected")],
    "risk_buffer": 0.3
}

# Generate scenarios
DF_DEM_SCENARIOS, scenario_arrays = gen_demand_scenarios(station_codes=sim_params["station_codes_YY"],
                                                    days_range=sim_params["scen_days"],
                                                    num_scenarios=sim_params["num_scenarios"],
                                                    confidence_interval=sim_params["confidence_interval"],
                                                    percentile_bandwidths=(sim_params["percentile_ranges_YIN"], sim_params["percentile_ranges_YANG"]),
                                                    retain_logics=sim_params["retain_logics"]) # logic options: raw, corrected,
                                                    # and scen (uncorrected demands, outlier corrected demands, or scenario generated demands)

# Modify scenario - YIN: reduce returns by 60%
# TODO: Have the modify_demands function generate a name for this scenario
modify_demands(df=DF_DEM_SCENARIOS,
                time_range=("00:00", "10:00"),
                reduction_percent=0,
                operation="reduce",
                handle_rentals=True,
                handle_returns=False,
                rentals_col="Rentals scenario YIN",
                returns_col="Returns scenario YIN")

modify_demands(df=DF_DEM_SCENARIOS,
                time_range=("00:00", "10:00"),
                reduction_percent=0,
                operation="reduce",
                handle_rentals=True,
                handle_returns=False,
                rentals_col="Rentals scenario YANG",
                returns_col="Returns scenario YANG")

# Prepare demands for forecast input
df_denorm, DF_DEM_NORM, models_YIN, models_YANG, scalers = prepare_demands_for_forecast(station_codes=sim_params["station_codes_YY"],
                                                                                        days_range=sim_params["test_days"],
                                                                                        month=sim_params["test_my"][0],
                                                                                        year=sim_params["test_my"][1],
                                                                                        scenario_sync=sim_params["sync_fcdem_scen"],
                                                                                        scenario_df=DF_DEM_SCENARIOS)

# Visualize a chosen day
chosen_day = 24

df_scenario_plot = DF_DEM_SCENARIOS.loc[DF_DEM_SCENARIOS.index.day == chosen_day]
plot_data_compare(df=df_scenario_plot,
                original_col_pair=("Rentals original YIN", "Returns original YIN"),
                adjusted_col_pair=("Rentals scenario YIN", "Returns scenario YIN"),
                subtitles=("Rentals", "Returns"),
                title=f"Demands: Original vs Scenarios at <b>{sim_params['station_codes_YY'][0]}</b>")
plot_data_compare(df=df_scenario_plot,
                original_col_pair=("Rentals original YANG", "Returns original YANG"),
                adjusted_col_pair=("Rentals scenario YANG", "Returns scenario YANG"),
                subtitles=("Rentals", "Returns"),
                title=f"Demands: Original vs Scenarios at <b>{sim_params['station_codes_YY'][1]}</b>")

visualize_demand_scenarios(df_scenario_plot, ("Rentals scenario YIN", "Returns scenario YIN"),
                           scenario_arrays[0][0], scenario_arrays[0][1], f"Demand scenarios for <b>{sim_params['station_codes_YY'][0]}</b>")

visualize_demand_scenarios(df_scenario_plot, ("Rentals scenario YANG", "Returns scenario YANG"),
                           scenario_arrays[1][0], scenario_arrays[1][1], f"Demand scenarios for <b>{sim_params['station_codes_YY'][1]}</b>")

# Simulation file paths
la_sim_path = SIM_MODELS_FOLDER + "PySD Projected stock optim model incl UPoA EBAF 1204.mdl"
yy_sim_path = SIM_MODELS_FOLDER + "Dual cluster actual stock simulation E16_E16I 0909.mdl"

# Load and initialize simulations; set models and scalers to global variables
model_ren_YIN, model_ret_YIN = models_YIN
model_ren_YANG, model_ret_YANG = models_YANG
scaler_YIN, scaler_YANG = scalers
vs_model_lookahead, vs_model_actual_yy = init_sims(la_sim_path, yy_sim_path, sim_params["risk_buffer"])
MODELS_DICT["LOOKAHEAD_YIN"] = (vs_model_lookahead, model_ren_YIN, model_ret_YIN, scaler_YIN)
MODELS_DICT["LOOKAHEAD_YANG"] = (vs_model_lookahead, model_ren_YANG, model_ret_YANG, scaler_YANG)
MODELS_DICT["ACTUAL_YIN_YANG"] = (vs_model_actual_yy, model_ren_YIN, model_ret_YIN, scaler_YIN)

"""### Generate scenario parameters"""

def generate_param_combinations(base_params, vary_params, output=False):
    keys, values = zip(*vary_params.items())
    variations = [dict(zip(keys, v)) for v in itertools.product(*values)]

    param_combinations = []
    for variation in variations:
        params = deepcopy(base_params)
        # Update the tuple within the parameters
        if 'risk_delay' in variation or 'avail_delay' in variation:
            risk_delay = variation.get('risk_delay', base_params["delays_YIN"][0])
            avail_delay = variation.get('avail_delay', base_params["delays_YIN"][1])
            params['delays_YIN'] = (risk_delay, avail_delay)
            params['delays_YANG'] = (risk_delay, avail_delay)

        params.update({k: v for k, v in variation.items() if k not in ['risk_delay', 'avail_delay']})
        param_combinations.append(params)

    if output:
        for params in param_combinations:
            print(params)

    return param_combinations

def get_scenario_key(vary_param_names, vary_param_values, scen_key_prefix):
    keys, _ = zip(*vary_param_names.items())
    scenario_key = scen_key_prefix
    for idx, k in enumerate(keys):
        if idx > 0:
            scenario_key += ", "
        if k == "risk_delay":
            scenario_key += f"{k}:{vary_param_values['delays_YIN'][0]}"
        elif k == "avail_delay":
            scenario_key += f"{k}:{vary_param_values['delays_YIN'][1]}"
        else:
            scenario_key += f"{k}:{vary_param_values[k]}"

    return scenario_key

# Model control variables
init_stocks = (0, 0)
master_params = {
    "sim_day": [22, 23, 24, 25],
    "init_stocks": init_stocks, # YIN, YANG
    "use_incentives": 0,
    "use_attractiveness": 1,
    "incentive_threshold": 0.8,
    "delays_YIN": (3, 3), # risk, availability
    "delays_YANG": (3, 3),
    "agg_logic": "MA",
    "agg_periods": 3
}

# Base parameters
base_params = {
    "sim_day": [24],
    "init_stocks": [(0, 0)],
    "risk_delay": [6],
    "avail_delay": [6],
    "agg_logic": ["MA"],
    "agg_periods": [12],
    "fcupd_freq": [11],
    "fcb_logic": [True]
}
# Parameters to vary to generate scenarios for simulation
vary_params = {
    "sim_day": [24],
    "init_stocks": [(0, 0)],
    "use_incentives": [1],
    "incentive_threshold": [0.8],
    "risk_delay": [6],
    "avail_delay": [6],
    "agg_logic": ["MA"],
    "agg_periods": [12],
    "fcupd_freq": [11],
    "fcb_logic": [True]
}

filter = "all" if len(vary_params["sim_day"]) > 1 else f"D{vary_params['sim_day'][0]}"
filter_suffix = "_ReDELAY_2"
scen_key_prefix = f"filter:{filter}{filter_suffix}, dem_scen:{sim_params['retain_logics'][0][0]}, risk_buffer:{sim_params['risk_buffer']}, "

# Generate parameters set: vary_params specifies the parameters to vary. The result is a list of parameters, each item an input for one simulation
control_vars_base = generate_param_combinations(master_params, base_params)
if vary_params:
  control_vars_additional = generate_param_combinations(master_params, vary_params)
  control_vars_set = control_vars_base + control_vars_additional
else:
  control_vars_set = control_vars_base

for i, control_vars in enumerate(control_vars_set):
  print(f"{i+1}. {control_vars}; Key: {get_scenario_key(vary_params if vary_params else base_params, control_vars, scen_key_prefix)}")

"""### Run coordinated simulations"""

# Run simulations
verbose = False
FCB_VERBOSE = False

scenario_frames = []
for control_vars in tqdm(control_vars_set, desc="Processing scenarios", position=0, leave=True):
  scenario_key = get_scenario_key(vary_params if vary_params else base_params, control_vars, scen_key_prefix)

  # (Re-)Initialize lookahead bike availability factor series
  PBAF_AGG_YIN = pd.Series()
  PBAF_AGG_YANG = pd.Series()

  # Fetch relevant data frame for the simulation day and run the coordinated YIN-YANG simulation
  day = control_vars["sim_day"]
  DF_DEM_PREDS_SIM_DAY = init_results_df(day, df_denorm)
  PRED_TARGETS_SIM_DAY = DF_DEM_PREDS_SIM_DAY.index

  INIT_PBAF_YIN = control_vars["init_stocks"][0]/STATION_CAPACITY[sim_params["station_codes_YY"][0]]
  INIT_PBAF_YANG = control_vars["init_stocks"][1]/STATION_CAPACITY[sim_params["station_codes_YY"][1]]
  init_pba_factors = (INIT_PBAF_YIN, INIT_PBAF_YANG)


# Simulation-related functions
  n_batches = control_vars["fcupd_freq"] + 1 # Number of splits of a day to be simulated
  batch_size = int(DAY_LEN / n_batches) # lookahead horizon

  # FC substitution logic on?
  if control_vars["fcb_logic"]:
    FCB_PROCESSING = True
  else:
    FCB_PROCESSING = False

  with tqdm(total=n_batches, desc="Batched simulation runs", leave=False, position=1) as tqdm_bar:
    DF_DEM_PREDS_SIM_DAY = run_coordinated_simulation(
                              station_codes=sim_params["station_codes_YY"],
                              la_model=vs_model_lookahead, yy_model=vs_model_actual_yy,
                              batch_size=batch_size, n_batches=n_batches,
                              delays_YIN=control_vars["delays_YIN"],
                              delays_YANG=control_vars["delays_YANG"],
                              use_attractiveness=control_vars["use_attractiveness"],
                              use_incentives=control_vars["use_incentives"],
                              incentive_threshold=control_vars["incentive_threshold"],
                              init_stocks=control_vars["init_stocks"],
                              init_pba_factors=init_pba_factors,
                              risk_buffer=sim_params["risk_buffer"],
                              agg_logic=control_vars["agg_logic"],
                              agg_periods=control_vars["agg_periods"],
                              df_results=DF_DEM_PREDS_SIM_DAY,
                              tqdm_bar=tqdm_bar,
                              verbose=verbose)
  DF_DEM_PREDS_SIM_DAY["scenario_key"] = scenario_key
  scenario_frames.append(DF_DEM_PREDS_SIM_DAY)

  df_scenarios_consolidated = pd.concat(scenario_frames, axis=0)

def merge_csv_files(file_paths, target_file_path):
    df_list = []

    for file_path in file_paths:
        df = pd.read_csv(file_path, skip_blank_lines=True, parse_dates=["Date"])
        df_list.append(df)

    merged_df = pd.concat(df_list)
    merged_df.to_csv(target_file_path, index=False)

if SAVE_SCENARIO_DEMANDS:
  cols_to_save = ["Rentals scenario YIN", "Returns scenario YIN", "Rentals scenario YANG", "Returns scenario YANG"]
  df_scen_sim = DF_DEM_SCENARIOS.loc[DF_DEM_SCENARIOS.index.day.isin(sim_params["scen_days"])][cols_to_save]
  df_scen_sim.to_csv(SIM_MODELS_FOLDER + "Outputs/demand_scenarios.csv")

if SAVE_PBAF:
  PBAF_AGG_YIN.to_csv(SIM_MODELS_FOLDER + "pbaf_e16.csv")
  PBAF_AGG_YANG.to_csv(SIM_MODELS_FOLDER + "pbaf_e16i.csv")

if SAVE_SCENARIO_RESULTS:
  timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
  df_scenarios_consolidated.to_csv(f"{SIM_MODELS_FOLDER}Outputs/scenarios_consolidated_single_{scen_key_prefix.rstrip(' ,')}_{timestamp}.csv")

if PLOT_DASHBOARDS:
  plot_dashboard(data=df_scenarios_consolidated,
                metrics_groups=[["Out of Stock of bikes", "Out of Stock of bikes at Y"],
                                ["Perceived bike availability factor", "Perceived bike availability factor at Y"],
                                ["Bike availability factor", "Bike availability factor at Y",
                                  "Projected bike availability factor", "Projected bike availability factor at Y"],
                                ["Perception of risk of OOS due to bike unavailability",
                                  "Perception of risk of OOS due to bike unavailability at Y"],
                                ["Change to returns rate due to incentives",
                                  "Change to returns rate due to incentives at Y"]],
                y_axis_labels=["Number of bikes or docks", "Availability as a % of capacity", "Availability as a % of capacity",
                                "Risk perception", "Incentive demand in units per period"],
                tick_interval="hourly",
                title=f"Simulation of <b>{sim_params['station_codes_YY'][0]} (YIN) - {sim_params['station_codes_YY'][1]} (YANG)</b>")

if COLLATE_SCENARIO_OUTPUTS:
  files_path = SIM_MODELS_FOLDER + "Outputs/"
  files_patt = f"scenarios_consolidated_single_*.csv"

  scenario_files = glob.glob(files_path + files_patt)
  timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
  scenario_files_collated = f"{files_path}scenarios_consolidated_multi_{timestamp}.csv"

  merge_csv_files(scenario_files, scenario_files_collated)

"""## Scratch"""

"""
cols = ["Bike availability factor",
        "Projected bike availability factor",
        "Discrepancy to actual",
        "Discrepancy to expected",
        "Update perception",
        "Perceived bike availability factor",
        "Perceived bike availability factor less GLB",
        "Perception of risk of OOS due to bike unavailability",
        "Perception of risk of OOS due to bike unavailability at Y",
        "Change to returns rate due to incentives at Y",
        "Expected returns uplift at Y"]
"""
#cols = ["Perceived bike availability factor",
#        "Perceived bike availability factor less GLB"]
#df_scenarios_consolidated.loc["2023-08-23 09:00":"2023-08-23 09:30", cols]

